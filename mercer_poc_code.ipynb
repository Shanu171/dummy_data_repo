#!/usr/bin/env python3
"""
pmi_modeling_fixed.py

Cleaned, robust pipeline for:
- EDA (correlation)
- Feature engineering (member-level)
- XGBoost classifier + regressor
- SHAP explainability (safe sampling)
Outputs saved to model_outputs/
"""

import os
import warnings
import random
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,
    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix
)
import xgboost as xgb

# shap import may be heavy; import last and handle errors
try:
    import shap
    HAS_SHAP = True
except Exception:
    HAS_SHAP = False

warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", 200)
random.seed(42)
np.random.seed(42)

# -----------------------------
# Parameters
# -----------------------------
DATA_DIR = "."  # change if CSVs are elsewhere
MEMBERSHIP_CSV = os.path.join(DATA_DIR, "membership_10k_realistic.csv")
CLAIMS_CSV = os.path.join(DATA_DIR, "claims_200k_realistic.csv")

PRED_START = pd.to_datetime("2023-01-01")
PRED_END = pd.to_datetime("2023-12-31")
HIST_CUTOFF = PRED_START - pd.Timedelta(days=1)
HIGH_CLAIM_THRESHOLD = 10000.0

OUT_DIR = os.path.join(DATA_DIR, "model_outputs")
os.makedirs(OUT_DIR, exist_ok=True)

RND = 42

# -----------------------------
# Utility functions
# -----------------------------
def read_csv_safe(path, parse_dates=None):
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")
    return pd.read_csv(path, parse_dates=parse_dates)

def ensure_datetime(df, col):
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce")
    else:
        df[col] = pd.NaT

def safe_savefig(figpath):
    try:
        plt.savefig(figpath, dpi=150, bbox_inches="tight")
    except Exception as e:
        print("Warning: could not save figure", figpath, str(e))

def extract_postcode_area(pc):
    try:
        s = str(pc)
        return s.split(" ")[0]
    except Exception:
        return "UNK"

# -----------------------------
# Load data
# -----------------------------
print("Loading files...")
membership = read_csv_safe(MEMBERSHIP_CSV)
claims = read_csv_safe(CLAIMS_CSV)

print(f"Membership rows: {len(membership):,}, Claims rows: {len(claims):,}")

# normalize columns
membership.columns = [c.strip() for c in membership.columns]
claims.columns = [c.strip() for c in claims.columns]

# Ensure key column exists
if "Unique Member Reference" not in membership.columns or "Unique Member Reference" not in claims.columns:
    raise KeyError("Required key 'Unique Member Reference' not found in one of the files.")

# Parse key dates defensively
for col in ["Contract Start Date", "Contract End Date", "Original Date of Joining", "Scheme Policy Joining Date", "Lapse Date"]:
    ensure_datetime(membership, col)

for col in ["Admission Date", "Discharge Date", "Incurred Date", "Paid Date"]:
    ensure_datetime(claims, col)

# Keep claims up to PRED_END (we only need history and prediction-year claims)
claims_hist = claims[claims["Admission Date"] <= PRED_END].copy()
print(f"Claims up to {PRED_END.date()}: {len(claims_hist):,}")

# -----------------------------
# Build targets (prediction year)
# -----------------------------
mask_target = (claims_hist["Admission Date"] >= PRED_START) & (claims_hist["Admission Date"] <= PRED_END)
claims_target = claims_hist[mask_target].copy()
print(f"Claims in target year ({PRED_START.date()} - {PRED_END.date()}): {len(claims_target):,}")

# ensure numeric Claim Amount
if "Claim Amount" not in claims_target.columns:
    raise KeyError("Claim Amount column not found in claims file.")
claims_target["Claim Amount"] = pd.to_numeric(claims_target["Claim Amount"], errors="coerce").fillna(0.0)

target_agg = claims_target.groupby("Unique Member Reference")["Claim Amount"].agg(
    target_high_claim_count=lambda s: (s >= HIGH_CLAIM_THRESHOLD).sum(),
    target_high_claim_amount=lambda s: s[s >= HIGH_CLAIM_THRESHOLD].sum()
).reset_index()

members = membership[["Unique Member Reference"]].copy()
target_df = members.merge(target_agg, on="Unique Member Reference", how="left").fillna(0.0)
target_df["target_has_high_claim"] = (target_df["target_high_claim_count"] >= 1).astype(int)

target_df.to_csv(os.path.join(OUT_DIR, "target_summary_2023.csv"), index=False)
print("Saved target_summary_2023.csv")

# -----------------------------
# Feature engineering (history <= HIST_CUTOFF)
# -----------------------------
hist_claims = claims_hist[claims_hist["Admission Date"] <= HIST_CUTOFF].copy()
print(f"Historical claims (<= {HIST_CUTOFF.date()}): {len(hist_claims):,}")

# numeric amounts
hist_claims["Claim Amount"] = pd.to_numeric(hist_claims.get("Claim Amount", 0), errors="coerce").fillna(0.0)
hist_claims["Amount Paid"] = pd.to_numeric(hist_claims.get("Amount Paid", 0), errors="coerce").fillna(0.0)

# basic aggregations
agg_funcs = {
    "Claim Amount": ["count", "sum", "mean", "median", "max"],
    "Amount Paid": ["sum", "mean"]
}
hist_agg = hist_claims.groupby("Unique Member Reference").agg(agg_funcs)
hist_agg.columns = ["_".join(x).strip() for x in hist_agg.columns.values]
hist_agg = hist_agg.reset_index().rename(columns={
    "Claim Amount_count": "hist_claim_count",
    "Claim Amount_sum": "hist_claim_amount_sum",
    "Claim Amount_mean": "hist_claim_amount_mean",
    "Claim Amount_median": "hist_claim_amount_median",
    "Claim Amount_max": "hist_claim_amount_max",
    "Amount Paid_sum": "hist_paid_sum",
    "Amount Paid_mean": "hist_paid_mean"
})

# historical high claims
hist_high = hist_claims[hist_claims["Claim Amount"] >= HIGH_CLAIM_THRESHOLD].groupby("Unique Member Reference")["Claim Amount"].agg(
    hist_high_claim_count="count", hist_high_claim_amount_sum="sum"
).reset_index()

# top conditions pivot (limit to top 12 to control width)
top_conditions = hist_claims["Condition Category"].value_counts().nlargest(12).index.tolist()
cond_counts = hist_claims.groupby(["Unique Member Reference", "Condition Category"]).size().unstack(fill_value=0)
# if any top condition missing, add zero column
for c in top_conditions:
    if c not in cond_counts.columns:
        cond_counts[c] = 0
cond_counts_reduced = cond_counts[top_conditions].reset_index().rename(columns={c: f"cond_cnt_{c}" for c in top_conditions})

# claim type counts
ctype_counts = hist_claims.groupby(["Unique Member Reference", "Claim Type"]).size().unstack(fill_value=0).reset_index()
ctype_cols = ["Unique Member Reference"] + [f"ctype_cnt_{c}" for c in ctype_counts.columns[1:]]
ctype_counts.columns = ctype_cols

# days since last claim
last_claim = hist_claims.groupby("Unique Member Reference")["Admission Date"].max().reset_index()
last_claim["days_since_last_claim"] = (HIST_CUTOFF - last_claim["Admission Date"]).dt.days
last_claim = last_claim[["Unique Member Reference", "days_since_last_claim"]]

# merge features
features = members.merge(hist_agg, on="Unique Member Reference", how="left")
features = features.merge(hist_high, on="Unique Member Reference", how="left")
features = features.merge(cond_counts_reduced, on="Unique Member Reference", how="left")
features = features.merge(ctype_counts, on="Unique Member Reference", how="left")
features = features.merge(last_claim, on="Unique Member Reference", how="left")
features.fillna(0, inplace=True)

# add membership meta
member_cols = ["Unique Member Reference", "Client Identifier", "Client Name", "Scheme Category/Section Name",
               "Year of Birth", "Gender", "Short Post Code of Member", "Status of Member", "Contract Start Date"]
for c in member_cols:
    if c not in membership.columns:
        membership[c] = np.nan

meta = membership[member_cols].copy()
# compute age & tenure
meta["Year of Birth"] = pd.to_numeric(meta["Year of Birth"], errors="coerce")
meta["age_at_cutoff"] = HIST_CUTOFF.year - meta["Year of Birth"]
meta["contract_start"] = pd.to_datetime(meta["Contract Start Date"], errors="coerce")
meta["tenure_days"] = (HIST_CUTOFF - meta["contract_start"]).dt.days.clip(lower=0).fillna(0)

features = features.merge(meta.drop(columns=["Contract Start Date"]), on="Unique Member Reference", how="left")
features.fillna(0, inplace=True)

# -----------------------------
# Final dataframe & correlation
# -----------------------------
df = features.merge(target_df, on="Unique Member Reference", how="left")
df.fillna(0, inplace=True)
df.to_csv(os.path.join(OUT_DIR, "features_with_target.csv"), index=False)
print("Saved features_with_target.csv")

# correlation matrix for numeric features
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
corr = df[numeric_cols].corr()
corr.to_csv(os.path.join(OUT_DIR, "correlation_matrix.csv"))
plt.figure(figsize=(10, 8))
sns.heatmap(corr.abs(), cmap="viridis")
plt.title("Absolute Correlation Matrix")
safe_savefig(os.path.join(OUT_DIR, "corr_matrix.png"))
plt.close()

# -----------------------------
# Prepare X, y
# -----------------------------
y = df["target_has_high_claim"].astype(int)
y_reg = df["target_high_claim_amount"].astype(float)

# drop identifiers & leakage
drop_cols = ["Unique Member Reference", "Client Name", "target_high_claim_count", "target_high_claim_amount", "target_has_high_claim"]
X = df.drop(columns=[c for c in drop_cols if c in df.columns])

# create derived categorical / numeric reductions
if "Short Post Code of Member" in X.columns:
    X["postcode_area"] = X["Short Post Code of Member"].fillna("UNK").apply(extract_postcode_area)
else:
    X["postcode_area"] = "UNK"

X["gender_num"] = X.get("Gender", "UNK").map({"M": 0, "F": 1, "B": 2}).fillna(-1)

# client frequency top-20
if "Client Identifier" in X.columns:
    top_clients = X["Client Identifier"].value_counts().nlargest(20).index.tolist()
    X["client_top"] = X["Client Identifier"].apply(lambda x: x if x in top_clients else "OTHER_CLIENT")
else:
    X["client_top"] = "OTHER_CLIENT"

X["scheme_cat"] = X.get("Scheme Category/Section Name", "UNK").astype(str).fillna("UNK")

# drop raw ones to avoid object columns
for c in ["Short Post Code of Member", "Gender", "Client Identifier", "Scheme Category/Section Name"]:
    if c in X.columns:
        X.drop(columns=[c], inplace=True)

# one-hot a few low-cardinality nominal fields
ohe_cols = ["client_top", "scheme_cat", "postcode_area"]
for c in ohe_cols:
    if c not in X.columns:
        X[c] = "UNK_" + c

X = pd.get_dummies(X, columns=ohe_cols, drop_first=True, dummy_na=False)

# Final numeric conversion (force numeric)
# --- CLEAN ALL COLUMNS TO PURE NUMERIC ---
def clean_numeric(val):
    if isinstance(val, (int, float, np.number)):
        return val
    if isinstance(val, str):
        # Remove brackets and commas
        val = val.replace("[", "").replace("]", "").replace(",", "")
        try:
            return float(val)
        except Exception:
            return np.nan
    return np.nan

# Apply cleaner column-wise
for c in X.columns:
    X[c] = X[c].apply(clean_numeric)

# Convert all to float and fill NaNs
X = X.astype(float).fillna(0.0)


print("Final feature matrix shape:", X.shape)

# safety for stratify: if y has only one class, do not stratify
stratify_param = y if y.nunique() > 1 else None

X_train, X_test, y_train, y_test, yreg_train, yreg_test = train_test_split(
    X, y, y_reg, test_size=0.2, random_state=RND, stratify=stratify_param
)
print(f"Train/test: {X_train.shape}, {X_test.shape}")

# -----------------------------
# Classifier
# -----------------------------
clf = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RND,
    use_label_encoder=False,
    eval_metric="logloss",
    verbosity=1
)

print("Training classifier...")
clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=30, verbose=50)

y_pred_proba = clf.predict_proba(X_test)[:, 1]
y_pred = (y_pred_proba >= 0.5).astype(int)

# metrics (guard for trivial case)
try:
    roc = roc_auc_score(y_test, y_pred_proba) if y_test.nunique() > 1 else np.nan
except Exception:
    roc = np.nan
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)

print(f"Classifier metrics: ROC AUC={roc}, Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}")
cm = confusion_matrix(y_test, y_pred)
print("Confusion matrix:\n", cm)

clf.save_model(os.path.join(OUT_DIR, "xgb_classifier.model"))

fi = pd.DataFrame({"feature": X.columns, "importance": clf.feature_importances_}).sort_values("importance", ascending=False)
fi.to_csv(os.path.join(OUT_DIR, "clf_feature_importance.csv"), index=False)

# SHAP for classifier (if available)
if HAS_SHAP:
    try:
        explainer = shap.TreeExplainer(clf)
        sample_size = min(1500, len(X_test))
        X_test_small = X_test.sample(n=sample_size, random_state=RND)
        shap_values = explainer.shap_values(X_test_small)
        shap.summary_plot(shap_values, X_test_small, show=False)
        safe_savefig(os.path.join(OUT_DIR, "shap_summary_classifier.png"))
        plt.close()

        # dependence plot for top feature
        if not fi.empty:
            top_feat = fi.iloc[0]["feature"]
            try:
                shap.dependence_plot(top_feat, shap_values, X_test_small, show=False)
                safe_savefig(os.path.join(OUT_DIR, f"shap_dependence_{top_feat}.png"))
                plt.close()
            except Exception:
                pass

        # save reduced shap values
        pd.DataFrame(shap_values, columns=X_test_small.columns).to_csv(os.path.join(OUT_DIR, "shap_values_sample.csv"), index=False)
        X_test_small.reset_index(drop=True).to_csv(os.path.join(OUT_DIR, "X_test_shap_sample.csv"), index=False)
    except Exception as e:
        print("SHAP failed:", e)
else:
    print("shap not installed or failed to import; skip SHAP step.")

# -----------------------------
# Regressor for high-claim amount
# -----------------------------
reg = xgb.XGBRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RND
)

print("Training regressor...")
reg.fit(X_train, yreg_train, eval_set=[(X_train, yreg_train), (X_test, yreg_test)], early_stopping_rounds=30, verbose=50)

yreg_pred = reg.predict(X_test)
mse = mean_squared_error(yreg_test, yreg_pred)
mae = mean_absolute_error(yreg_test, yreg_pred)
r2 = r2_score(yreg_test, yreg_pred)
print(f"Regressor metrics: MSE={mse:.2f}, MAE={mae:.2f}, R2={r2:.4f}")

reg.save_model(os.path.join(OUT_DIR, "xgb_regressor.model"))
pd.DataFrame({"feature": X.columns, "importance": reg.feature_importances_}).sort_values("importance", ascending=False).to_csv(os.path.join(OUT_DIR, "reg_feature_importance.csv"), index=False)

# SHAP regressor
if HAS_SHAP:
    try:
        explainer_reg = shap.TreeExplainer(reg)
        sample_size = min(1500, len(X_test))
        X_test_small_reg = X_test.sample(n=sample_size, random_state=RND)
        shap_values_reg = explainer_reg.shap_values(X_test_small_reg)
        shap.summary_plot(shap_values_reg, X_test_small_reg, show=False)
        safe_savefig(os.path.join(OUT_DIR, "shap_summary_regressor.png"))
        plt.close()
    except Exception as e:
        print("SHAP (regressor) failed:", e)

# -----------------------------
# Save test predictions and metrics
# -----------------------------
test_results = X_test.copy()
test_results["y_true_has_high"] = y_test.values
test_results["y_pred_proba_has_high"] = y_pred_proba
test_results["y_pred_has_high"] = y_pred
test_results["y_true_high_amount"] = yreg_test.values
test_results["y_pred_high_amount"] = yreg_pred
test_results.to_csv(os.path.join(OUT_DIR, "test_predictions.csv"), index=False)

with open(os.path.join(OUT_DIR, "model_metrics.txt"), "w") as f:
    f.write("Classifier metrics:\n")
    f.write(f"ROC AUC: {roc}\nAccuracy: {acc:.4f}\nPrecision: {prec:.4f}\nRecall: {rec:.4f}\nF1: {f1:.4f}\n\n")
    f.write("Regressor metrics:\n")
    f.write(f"MSE: {mse:.2f}\nMAE: {mae:.2f}\nR2: {r2:.4f}\n")

print("All outputs saved to:", OUT_DIR)
