{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# --------------------------\n",
    "# 1️⃣ Generate Synthetic Data\n",
    "# --------------------------\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_members = 2000\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime(2025, 1, 1)\n",
    "\n",
    "# Membership data\n",
    "members = pd.DataFrame({\n",
    "    \"member_id\": range(1, n_members + 1),\n",
    "    \"age\": np.random.randint(20, 75, n_members),\n",
    "    \"gender\": np.random.choice([\"M\", \"F\"], n_members),\n",
    "    \"plan_type\": np.random.choice([\"Silver\", \"Gold\", \"Platinum\"], n_members, p=[0.5, 0.3, 0.2]),\n",
    "    \"join_date\": [start_date + timedelta(days=np.random.randint(0, 365*5)) for _ in range(n_members)],\n",
    "})\n",
    "\n",
    "# Claim data\n",
    "n_claims = 10000\n",
    "claim_dates = [start_date + timedelta(days=np.random.randint(0, (end_date - start_date).days)) for _ in range(n_claims)]\n",
    "claims = pd.DataFrame({\n",
    "    \"claim_id\": range(1, n_claims + 1),\n",
    "    \"member_id\": np.random.choice(members[\"member_id\"], n_claims),\n",
    "    \"claim_date\": claim_dates,\n",
    "    \"claim_amount\": np.random.exponential(scale=15000, size=n_claims).astype(int)\n",
    "})\n",
    "\n",
    "# --------------------------\n",
    "# 2️⃣ Define high-claim threshold\n",
    "# --------------------------\n",
    "threshold = claims[\"claim_amount\"].quantile(0.9)\n",
    "claims[\"is_high_claim\"] = (claims[\"claim_amount\"] > threshold).astype(int)\n",
    "\n",
    "# --------------------------\n",
    "# 3️⃣ Create historical (past 2 years) features & future (next 2 years) targets\n",
    "# --------------------------\n",
    "\n",
    "cutoff_date = datetime(2021, 1, 1)  # pretend we are training as of Jan 2021\n",
    "past_window_start = cutoff_date - timedelta(days=365*2)\n",
    "future_window_end = cutoff_date + timedelta(days=365*2)\n",
    "\n",
    "# Past 2 years (feature window)\n",
    "past_claims = claims[(claims[\"claim_date\"] >= past_window_start) & (claims[\"claim_date\"] < cutoff_date)]\n",
    "# Future 2 years (target window)\n",
    "future_claims = claims[(claims[\"claim_date\"] >= cutoff_date) & (claims[\"claim_date\"] < future_window_end)]\n",
    "\n",
    "# Aggregate past claims\n",
    "past_features = past_claims.groupby(\"member_id\").agg(\n",
    "    past_claim_count=(\"claim_id\", \"count\"),\n",
    "    past_total_amount=(\"claim_amount\", \"sum\"),\n",
    "    past_high_claims=(\"is_high_claim\", \"sum\"),\n",
    "    avg_claim_amount=(\"claim_amount\", \"mean\"),\n",
    "    last_claim_date=(\"claim_date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Fill missing (members with no past claims)\n",
    "past_features[\"days_since_last_claim\"] = (cutoff_date - past_features[\"last_claim_date\"]).dt.days\n",
    "past_features.drop(columns=\"last_claim_date\", inplace=True)\n",
    "past_features.fillna({\"days_since_last_claim\": 9999, \"avg_claim_amount\": 0,\n",
    "                      \"past_claim_count\": 0, \"past_total_amount\": 0, \"past_high_claims\": 0}, inplace=True)\n",
    "\n",
    "# Target: future high claim count & amount\n",
    "future_targets = future_claims[future_claims[\"is_high_claim\"] == 1].groupby(\"member_id\").agg(\n",
    "    future_high_claim_count=(\"claim_id\", \"count\"),\n",
    "    future_high_claim_amount=(\"claim_amount\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Merge features + targets + member info\n",
    "data = members.merge(past_features, on=\"member_id\", how=\"left\").merge(future_targets, on=\"member_id\", how=\"left\")\n",
    "data.fillna({\"future_high_claim_count\": 0, \"future_high_claim_amount\": 0}, inplace=True)\n",
    "\n",
    "# --------------------------\n",
    "# 4️⃣ Encode categorical variables\n",
    "# --------------------------\n",
    "data = pd.get_dummies(data, columns=[\"gender\", \"plan_type\"], drop_first=True)\n",
    "\n",
    "# --------------------------\n",
    "# 5️⃣ Split Train/Test\n",
    "# --------------------------\n",
    "X = data.drop(columns=[\"member_id\", \"future_high_claim_count\", \"future_high_claim_amount\", \"join_date\"])\n",
    "y_count = data[\"future_high_claim_count\"]\n",
    "y_amount = data[\"future_high_claim_amount\"]\n",
    "\n",
    "X_train, X_test, y_count_train, y_count_test, y_amount_train, y_amount_test = train_test_split(\n",
    "    X, y_count, y_amount, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 6️⃣ Train Models\n",
    "# --------------------------\n",
    "count_model = XGBRegressor(objective='count:poisson', n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "count_model.fit(X_train, y_count_train)\n",
    "\n",
    "amount_model = XGBRegressor(objective='reg:squarederror', n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "amount_model.fit(X_train, y_amount_train)\n",
    "\n",
    "# --------------------------\n",
    "# 7️⃣ Evaluate Models\n",
    "# --------------------------\n",
    "count_pred = count_model.predict(X_test)\n",
    "amount_pred = amount_model.predict(X_test)\n",
    "\n",
    "print(\"=== Claim COUNT Model ===\")\n",
    "print(\"MAE:\", mean_absolute_error(y_count_test, count_pred))\n",
    "print(\"R² :\", r2_score(y_count_test, count_pred))\n",
    "\n",
    "print(\"\\n=== Claim AMOUNT Model ===\")\n",
    "print(\"MAE:\", mean_absolute_error(y_amount_test, amount_pred))\n",
    "print(\"R² :\", r2_score(y_amount_test, amount_pred))\n",
    "\n",
    "# --------------------------\n",
    "# 8️⃣ Predict future for all members\n",
    "# --------------------------\n",
    "data[\"pred_future_high_claim_count\"] = count_model.predict(X)\n",
    "data[\"pred_future_high_claim_amount\"] = amount_model.predict(X)\n",
    "data[\"predicted_total_high_claim_cost\"] = data[\"pred_future_high_claim_count\"] * data[\"pred_future_high_claim_amount\"]\n",
    "\n",
    "# Display sample output\n",
    "print(\"\\nSample predictions:\")\n",
    "print(data[[\"member_id\", \"pred_future_high_claim_count\", \"pred_future_high_claim_amount\", \"predicted_total_high_claim_cost\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e51886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmi_modeling.py\n",
    "\"\"\"\n",
    "Pipeline for EDA, feature engineering, XGBoost modeling, and SHAP explainability\n",
    "on the provided PMI synthetic datasets.\n",
    "\n",
    "Usage:\n",
    "  1. Place `membership_10k_realistic.csv` and `claims_200k_realistic.csv`\n",
    "     in the same folder as this script (or change DATA_DIR below).\n",
    "  2. Install required packages (see README section below).\n",
    "  3. Run: python pmi_modeling.py\n",
    "\n",
    "Outputs are saved into a subfolder `model_outputs` inside DATA_DIR.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters / file locations\n",
    "# -----------------------------\n",
    "DATA_DIR = \".\"  # change to folder where CSVs are; e.g. \"/path/to/csvs\"\n",
    "MEMBERSHIP_CSV = os.path.join(DATA_DIR, \"membership_10k_realistic.csv\")\n",
    "CLAIMS_CSV = os.path.join(DATA_DIR, \"claims_200k_realistic.csv\")\n",
    "\n",
    "# Prediction horizon: 2023 calendar year (modify if you want FY 2023-24)\n",
    "PRED_START = pd.to_datetime(\"2023-01-01\")\n",
    "PRED_END   = pd.to_datetime(\"2023-12-31\")\n",
    "\n",
    "# Feature history cutoff: use history BEFORE PRED_START\n",
    "HIST_CUTOFF = PRED_START - pd.Timedelta(days=1)\n",
    "\n",
    "# High claim threshold (GBP) — change if you want different definition\n",
    "HIGH_CLAIM_THRESHOLD = 10000.0\n",
    "\n",
    "# Where to save outputs\n",
    "OUT_DIR = os.path.join(DATA_DIR, \"model_outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RND = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Utility functions\n",
    "# -----------------------------\n",
    "def safe_read_csv(path, parse_dates=None):\n",
    "    print(f\"Reading {path} ...\")\n",
    "    return pd.read_csv(path, parse_dates=parse_dates)\n",
    "\n",
    "def print_shape(name, df):\n",
    "    print(f\"{name}: rows={len(df):,}, cols={df.shape[1]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. LOAD DATA\n",
    "# -----------------------------\n",
    "membership = safe_read_csv(MEMBERSHIP_CSV, parse_dates=[\"Contract Start Date\", \"Contract End Date\",\n",
    "                                                       \"Original Date of Joining\", \"Scheme Policy Joining Date\", \"Lapse Date\"])\n",
    "claims = safe_read_csv(CLAIMS_CSV, parse_dates=[\"Admission Date\",\"Discharge Date\",\"Incurred Date\",\"Paid Date\"])\n",
    "\n",
    "print_shape(\"membership\", membership)\n",
    "print_shape(\"claims\", claims)\n",
    "\n",
    "# Normalize column names (strip)\n",
    "membership.columns = [c.strip() for c in membership.columns]\n",
    "claims.columns = [c.strip() for c in claims.columns]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. PREPROCESS / CLEAN\n",
    "# -----------------------------\n",
    "assert \"Unique Member Reference\" in membership.columns\n",
    "assert \"Unique Member Reference\" in claims.columns\n",
    "\n",
    "for col in [\"Contract Start Date\",\"Contract End Date\",\"Original Date of Joining\",\"Scheme Policy Joining Date\",\"Lapse Date\"]:\n",
    "    if col in membership.columns:\n",
    "        membership[col] = pd.to_datetime(membership[col], errors=\"coerce\")\n",
    "\n",
    "for col in [\"Admission Date\",\"Discharge Date\",\"Incurred Date\",\"Paid Date\"]:\n",
    "    if col in claims.columns:\n",
    "        claims[col] = pd.to_datetime(claims[col], errors=\"coerce\")\n",
    "\n",
    "# Keep only claims up to the end of prediction horizon (we only need history + target)\n",
    "claims_hist = claims[claims[\"Admission Date\"] <= PRED_END].copy()\n",
    "print_shape(\"claims_hist (<= pred_end)\", claims_hist)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. BUILD TARGETS (prediction year)\n",
    "# -----------------------------\n",
    "mask_target = (claims_hist[\"Admission Date\"] >= PRED_START) & (claims_hist[\"Admission Date\"] <= PRED_END)\n",
    "claims_target = claims_hist[mask_target].copy()\n",
    "print_shape(\"claims_target (in prediction year)\", claims_target)\n",
    "\n",
    "claims_target[\"Claim Amount\"] = pd.to_numeric(claims_target[\"Claim Amount\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "target_agg = claims_target.groupby(\"Unique Member Reference\")[\"Claim Amount\"].agg(\n",
    "    target_high_claim_count=lambda s: (s >= HIGH_CLAIM_THRESHOLD).sum(),\n",
    "    target_high_claim_amount=lambda s: s[s >= HIGH_CLAIM_THRESHOLD].sum()\n",
    ").reset_index()\n",
    "\n",
    "members = membership[[\"Unique Member Reference\"]].copy()\n",
    "target_df = members.merge(target_agg, on=\"Unique Member Reference\", how=\"left\").fillna(0.0)\n",
    "target_df[\"target_has_high_claim\"] = (target_df[\"target_high_claim_count\"] >= 1).astype(int)\n",
    "\n",
    "print_shape(\"target_df\", target_df)\n",
    "target_df.to_csv(os.path.join(OUT_DIR, \"target_summary_2023.csv\"), index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. FEATURE ENGINEERING (use history before HIST_CUTOFF)\n",
    "# -----------------------------\n",
    "hist_claims = claims_hist[claims_hist[\"Admission Date\"] <= HIST_CUTOFF].copy()\n",
    "print_shape(\"hist_claims (<= hist_cutoff)\", hist_claims)\n",
    "\n",
    "hist_claims[\"Claim Amount\"] = pd.to_numeric(hist_claims[\"Claim Amount\"], errors=\"coerce\").fillna(0.0)\n",
    "hist_claims[\"Amount Paid\"] = pd.to_numeric(hist_claims.get(\"Amount Paid\", pd.Series(0, index=hist_claims.index)), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "agg_funcs = {\n",
    "    \"Claim Amount\": [\"count\", \"sum\", \"mean\", \"median\", \"max\"],\n",
    "    \"Amount Paid\": [\"sum\",\"mean\"]\n",
    "}\n",
    "hist_agg = hist_claims.groupby(\"Unique Member Reference\").agg(agg_funcs)\n",
    "hist_agg.columns = [\"_\".join(col).strip() for col in hist_agg.columns.values]\n",
    "hist_agg = hist_agg.reset_index().rename(columns={\n",
    "    \"Claim Amount_count\": \"hist_claim_count\",\n",
    "    \"Claim Amount_sum\": \"hist_claim_amount_sum\",\n",
    "    \"Claim Amount_mean\": \"hist_claim_amount_mean\",\n",
    "    \"Claim Amount_median\": \"hist_claim_amount_median\",\n",
    "    \"Claim Amount_max\": \"hist_claim_amount_max\",\n",
    "    \"Amount Paid_sum\": \"hist_paid_sum\",\n",
    "    \"Amount Paid_mean\": \"hist_paid_mean\"\n",
    "})\n",
    "\n",
    "hist_high = hist_claims[hist_claims[\"Claim Amount\"] >= HIGH_CLAIM_THRESHOLD].groupby(\"Unique Member Reference\")[\"Claim Amount\"].agg(\n",
    "    hist_high_claim_count=\"count\", hist_high_claim_amount_sum=\"sum\"\n",
    ").reset_index()\n",
    "\n",
    "# Condition counts pivot (top N conditions)\n",
    "top_conditions = hist_claims[\"Condition Category\"].value_counts().nlargest(15).index.tolist()\n",
    "cond_counts = hist_claims.groupby([\"Unique Member Reference\",\"Condition Category\"]).size().unstack(fill_value=0)\n",
    "for c in top_conditions:\n",
    "    if c not in cond_counts.columns:\n",
    "        cond_counts[c] = 0\n",
    "cond_counts_reduced = cond_counts[top_conditions].reset_index().rename(columns={c: f\"cond_cnt_{c}\" for c in top_conditions})\n",
    "\n",
    "# Claim type counts\n",
    "ctype_counts = hist_claims.groupby([\"Unique Member Reference\",\"Claim Type\"]).size().unstack(fill_value=0).reset_index()\n",
    "ctype_counts.columns = [\"Unique Member Reference\"] + [f\"ctype_cnt_{c}\" for c in ctype_counts.columns[1:]]\n",
    "\n",
    "# Recent activity: days since last claim before HIST_CUTOFF\n",
    "last_claim = hist_claims.groupby(\"Unique Member Reference\")[\"Admission Date\"].max().reset_index()\n",
    "last_claim[\"days_since_last_claim\"] = (HIST_CUTOFF - last_claim[\"Admission Date\"]).dt.days\n",
    "last_claim = last_claim[[\"Unique Member Reference\",\"days_since_last_claim\"]]\n",
    "\n",
    "# Merge features\n",
    "features = members.merge(hist_agg, on=\"Unique Member Reference\", how=\"left\")\n",
    "features = features.merge(hist_high, on=\"Unique Member Reference\", how=\"left\")\n",
    "features = features.merge(cond_counts_reduced, on=\"Unique Member Reference\", how=\"left\")\n",
    "features = features.merge(ctype_counts, on=\"Unique Member Reference\", how=\"left\")\n",
    "features = features.merge(last_claim, on=\"Unique Member Reference\", how=\"left\")\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "# Add member fields (demographics / scheme / client)\n",
    "member_cols_to_use = [\n",
    "    \"Unique Member Reference\",\"Client Identifier\",\"Client Name\",\"Scheme Category/Section Name\",\n",
    "    \"Year of Birth\",\"Gender\",\"Short Post Code of Member\",\"Status of Member\",\"Contract Start Date\",\"Contract End Date\"\n",
    "]\n",
    "meta = membership[member_cols_to_use].copy()\n",
    "meta[\"age_at_cutoff\"] = HIST_CUTOFF.astype('datetime64[Y]').astype(int) - meta[\"Year of Birth\"]\n",
    "meta[\"contract_start\"] = pd.to_datetime(meta[\"Contract Start Date\"], errors=\"coerce\")\n",
    "meta[\"tenure_days\"] = (HIST_CUTOFF - meta[\"contract_start\"]).dt.days.clip(lower=0).fillna(0)\n",
    "\n",
    "# merge meta\n",
    "features = features.merge(meta.drop(columns=[\"Contract Start Date\",\"Contract End Date\"]), on=\"Unique Member Reference\", how=\"left\")\n",
    "features.fillna(0, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. MERGE TARGET & FINALIZE FEATURE MATRIX\n",
    "# -----------------------------\n",
    "df = features.merge(target_df, on=\"Unique Member Reference\", how=\"left\")\n",
    "df.fillna(0, inplace=True)\n",
    "print_shape(\"model dataframe\", df)\n",
    "df.to_csv(os.path.join(OUT_DIR, \"features_with_target.csv\"), index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. SIMPLE EDA & CORRELATION\n",
    "# -----------------------------\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = df[numeric_cols].corr()\n",
    "corr.to_csv(os.path.join(OUT_DIR, \"correlation_matrix.csv\"))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr.abs(), cmap='viridis', cbar=True)\n",
    "plt.title(\"Absolute Correlation Matrix (numeric features)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"corr_matrix.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. PREPARE DATA FOR MODELING\n",
    "# -----------------------------\n",
    "y = df[\"target_has_high_claim\"].astype(int)\n",
    "y_reg = df[\"target_high_claim_amount\"].astype(float)\n",
    "\n",
    "drop_cols = [\n",
    "    \"Unique Member Reference\",\"Client Name\",\"Client Identifier\",\"Scheme Category/Section Name\",\n",
    "    \"target_high_claim_count\",\"target_high_claim_amount\",\"target_has_high_claim\"\n",
    "]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# Extract postcode area (outward) and encode basic demographics\n",
    "def extract_postcode_area(pc):\n",
    "    try:\n",
    "        return str(pc).split(\" \")[0]\n",
    "    except:\n",
    "        return \"UNK\"\n",
    "\n",
    "X[\"postcode_area\"] = X[\"Short Post Code of Member\"].fillna(\"UNK\").apply(extract_postcode_area)\n",
    "X[\"gender_num\"] = X[\"Gender\"].map({\"M\":0,\"F\":1,\"B\":2}).fillna(-1)\n",
    "\n",
    "# Frequency encode top clients\n",
    "top_clients = X[\"Client Identifier\"].value_counts().nlargest(20).index.tolist()\n",
    "X[\"client_top\"] = X[\"Client Identifier\"].apply(lambda x: x if x in top_clients else \"OTHER_CLIENT\")\n",
    "X[\"scheme_cat\"] = X[\"Scheme Category/Section Name\"].astype(str).fillna(\"UNK\")\n",
    "\n",
    "# Drop raw columns and one-hot small cardinality columns\n",
    "for c in [\"Short Post Code of Member\",\"Gender\",\"Client Identifier\",\"Scheme Category/Section Name\"]:\n",
    "    if c in X.columns:\n",
    "        X.drop(columns=[c], inplace=True)\n",
    "\n",
    "ohe_cols = [\"client_top\",\"scheme_cat\",\"postcode_area\"]\n",
    "X = pd.get_dummies(X, columns=ohe_cols, drop_first=True, dummy_na=False)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "print(\"Final feature matrix shape:\", X.shape)\n",
    "\n",
    "# Train/test split with stratify on rare target\n",
    "X_train, X_test, y_train, y_test, yreg_train, yreg_test = train_test_split(\n",
    "    X, y, y_reg, test_size=0.2, random_state=RND, stratify=y\n",
    ")\n",
    "print(\"Train/test sizes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. CLASSIFIER: XGBoost\n",
    "# -----------------------------\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=RND, use_label_encoder=False, eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_test,y_test)], verbose=50, early_stopping_rounds=30)\n",
    "\n",
    "# Predictions & metrics\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "roc = roc_auc_score(y_test, y_pred_proba)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "print(f\"Classifier ROC AUC: {roc:.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "clf.save_model(os.path.join(OUT_DIR, \"xgb_classifier.model\"))\n",
    "\n",
    "fi = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": clf.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "fi.to_csv(os.path.join(OUT_DIR, \"clf_feature_importance.csv\"), index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 9. SHAP EXPLAINABILITY (classifier)\n",
    "# -----------------------------\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "X_test_small = X_test.sample(n=min(2000, len(X_test)), random_state=RND)\n",
    "shap_values = explainer.shap_values(X_test_small)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_small, show=False)\n",
    "plt.title(\"SHAP summary (classifier)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"shap_summary_classifier.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "top_feat = fi.iloc[0][\"feature\"]\n",
    "try:\n",
    "    shap.dependence_plot(top_feat, shap_values, X_test_small, show=False)\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"shap_dependence_{top_feat}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(\"SHAP dependence plot failed for feature:\", top_feat, e)\n",
    "\n",
    "# Save SHAP values sample and corresponding X sample\n",
    "shap_df = pd.DataFrame(shap_values, columns=X_test_small.columns)\n",
    "shap_df.to_csv(os.path.join(OUT_DIR, \"shap_values_sample.csv\"), index=False)\n",
    "X_test_small.reset_index(drop=True).to_csv(os.path.join(OUT_DIR, \"X_test_shap_sample.csv\"), index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 10. REGRESSOR: XGBoost for high-claim amount\n",
    "# -----------------------------\n",
    "reg = xgb.XGBRegressor(\n",
    "    n_estimators=400, max_depth=6, learning_rate=0.05, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=RND\n",
    ")\n",
    "\n",
    "reg.fit(X_train, yreg_train, eval_set=[(X_train,yreg_train),(X_test,yreg_test)], verbose=50, early_stopping_rounds=30)\n",
    "\n",
    "yreg_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(yreg_test, yreg_pred)\n",
    "mae = mean_absolute_error(yreg_test, yreg_pred)\n",
    "r2 = r2_score(yreg_test, yreg_pred)\n",
    "print(f\"Regressor MSE={mse:.2f}, MAE={mae:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "reg.save_model(os.path.join(OUT_DIR, \"xgb_regressor.model\"))\n",
    "fi_reg = pd.DataFrame({\"feature\": X.columns, \"importance\": reg.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
    "fi_reg.to_csv(os.path.join(OUT_DIR, \"reg_feature_importance.csv\"), index=False)\n",
    "\n",
    "explainer_reg = shap.TreeExplainer(reg)\n",
    "X_test_small_reg = X_test.sample(n=min(2000,len(X_test)), random_state=RND)\n",
    "shap_values_reg = explainer_reg.shap_values(X_test_small_reg)\n",
    "\n",
    "shap.summary_plot(shap_values_reg, X_test_small_reg, show=False)\n",
    "plt.title(\"SHAP summary (regressor)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"shap_summary_regressor.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# 11. OUTPUTS & REPORT ASSETS\n",
    "# -----------------------------\n",
    "test_results = X_test.copy()\n",
    "test_results[\"y_true_has_high\"] = y_test.values\n",
    "test_results[\"y_pred_proba_has_high\"] = y_pred_proba\n",
    "test_results[\"y_pred_has_high\"] = y_pred\n",
    "test_results[\"y_true_high_amount\"] = yreg_test.values\n",
    "test_results[\"y_pred_high_amount\"] = yreg_pred\n",
    "test_results.to_csv(os.path.join(OUT_DIR, \"test_predictions.csv\"), index=False)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"model_metrics.txt\"), \"w\") as f:\n",
    "    f.write(\"Classifier metrics:\\n\")\n",
    "    f.write(f\"ROC AUC: {roc:.4f}\\nAccuracy: {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1: {f1:.4f}\\n\\n\")\n",
    "    f.write(\"Regressor metrics:\\n\")\n",
    "    f.write(f\"MSE: {mse:.2f}\\nMAE: {mae:.2f}\\nR2: {r2:.4f}\\n\")\n",
    "\n",
    "print(\"All outputs saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3b8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
